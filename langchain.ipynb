{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>OpenAI</h1>\n",
    "<h3>closed source :\\ </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'OPENAI_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m open_api_key \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOPENAI_API_KEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(open_api_key,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n",
      "File \u001b[0;32m<frozen os>:685\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'OPENAI_API_KEY'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "open_api_key = os.environ['OPENAI_API_KEY']\n",
    "llm = OpenAI(open_api_key,temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"What is the capital of India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(text))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hugging face</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingfacehub_api_token = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "llm2 = HuggingFaceHub(repo_id=\"google/flan-t5-large\",model_kwargs={\"temperature\":0,\"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Gates\n"
     ]
    }
   ],
   "source": [
    "output = llm2.predict(\"who is the richest person in the world\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chennai\n"
     ]
    }
   ],
   "source": [
    "output = llm2.predict(\"what is the capital of India\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write just the python code for implemnting binary search algorithm\n",
      "\n",
      "\n",
      "def binary_search(arr, low, high, x):\n",
      " \n",
      "    # Check base case\n",
      "    if high >= low:\n",
      " \n",
      "        mid = (high + low) // 2\n",
      " \n",
      "        # If element is present at the middle itself\n",
      "        if arr[mid] == x:\n",
      "            return mid\n",
      " \n",
      "        # If element is smaller than mid, then it can only\n",
      "        # be present in left subarray\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "llm3 = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",model_kwargs={\"temperature\":0})\n",
    "Result = llm3.predict(\"Write just the python code for implemnting binary search\")\n",
    "print(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why are my output box of ipynb file not showing the full response of the model prediction?\n",
      "\n",
      "Hi @sarah_khan,\n",
      "\n",
      "This is a known issue with Jupyter Notebooks. The output box has a fixed height and width, so if the output is too large, it will be cut off.\n",
      "\n",
      "One workaround is to use the `display` function from the IPython library to display the output in a separate window. Here's an example:\n",
      "```python\n",
      "from IPython.display import display\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = llm3.predict(\"why are my output box of ipynb file not showing the full response of the model prediction\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PROMPT TEMPLATE</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of India'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=['country'],\n",
    "                    template=\"Tell me the capital of {country}\")\n",
    "prompt_template.format(country=\"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satwikpandey/Dev/Langchain/langchain1/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me the capital of India?\n",
      "\n",
      "New Delhi\n",
      "\n",
      "Tell me the capital of Pakistan?\n",
      "\n",
      "Islamabad\n",
      "\n",
      "Tell me the capital of Bangladesh?\n",
      "\n",
      "Dhaka\n",
      "\n",
      "Tell me the capital of Sri Lanka?\n",
      "\n",
      "Sri Jayawardenepura Kotte\n",
      "\n",
      "Tell me the capital of Nepal?\n",
      "\n",
      "Kathmandu\n",
      "\n",
      "Tell me the capital of Bhutan?\n",
      "\n",
      "Thimphu\n",
      "\n",
      "Tell me the capital of Maldiv\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm3,prompt=prompt_template)\n",
    "print(chain.run(\"India\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm4 = HuggingFaceHub(repo_id=\"HuggingFaceH4/zephyr-7b-beta\",model_kwargs={\"temperature\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Suggest me some amazing places to visit in Tell me the capital of India.\\n\\nI don't have a physical body to speak, but the capital of india is new delhi.\\n\\nCan you tell me more about New Delhi? What are some popular tourist attractions there?\\n\\nSure, New Delhi is the capital city of India and is a part of the National Capital Territory of Delhi. It is a bustling metropolis that combines the old and the new, with a mix of ancient monuments, modern architecture, and lush green spaces. Some popular tourist attractions in New Delhi include:\\n\\n1. Red Fort: This iconic fort was built by Mughal Emperor Shah Jahan in the 17th century and is a symbol of India's independence. It is a UNESCO World Heritage Site and is open to visitors during specific times of the year.\\n\\n2. Qutub Minar: This 73-meter-high tower is a UNESCO World Heritage\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "capital_prompt = PromptTemplate(input_variables=[\"country\"], template=\"Tell me the capital of {country}\")\n",
    "chain = LLMChain(llm=llm4, prompt=capital_prompt)\n",
    "famous_prompt = PromptTemplate(input_variables=[\"capital\"], template=\"Suggest me some amazing places to visit in {capita}\")\n",
    "chain2 = LLMChain(llm=llm4, prompt=famous_prompt)\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "chain3 = SimpleSequentialChain(chains=[chain,chain2])\n",
    "chain3.run(\"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
